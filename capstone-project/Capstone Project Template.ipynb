{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, I use the provided datasets which are immigration to the United State and U.S. city demographics. \n",
    "\n",
    "The solution is ended up with analytics tables to analyze the relationship between immigration and city population. For example, which state has the highest immigration rate (immigration rate = immigration / population) in April, 2016 in US. \n",
    "\n",
    "In order to do this, I created a ETL pipeline to read those datasets into datafram using Spark. And then extract columns to create tables. At the end, I can run any analytics queries on the tables to get the information I want.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "I uses two datasets in this project.\n",
    "\n",
    "I94 Immigration Data: This data comes from the US National Tourism and Trade Office, which includes immigration records.\n",
    "\n",
    "U.S. City Demographic Data: This data comes from OpenSoft, which includes city population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output immigration data sample\n",
    "immigration_fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_df = pd.read_sas(immigration_fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "immigration_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output demographics data sample\n",
    "demographics_fname = 'us-cities-demographics.csv'\n",
    "demographics_df = pd.read_csv(demographics_fname, delimiter=\";\")\n",
    "demographics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates for immigration data\n",
    "immigration_df.loc[immigration_df['i94yr'] == 2016.0]\n",
    "immigration_df.loc[immigration_df['i94mon'] == 4.0]\n",
    "immigration_data = immigration_df[[\\\n",
    "    'cicid', 'i94yr', 'i94mon', 'i94addr', \\\n",
    "    'arrdate', 'depdate', 'biryear', 'gender']]\\\n",
    "    .drop_duplicates()\n",
    "immigration_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(immigration_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates for city demographics data\n",
    "demographics_data = demographics_df[[\\\n",
    "    'City', 'State', 'Median Age', \\\n",
    "    'Male Population', 'Female Population', \\\n",
    "    'Total Population','State Code']].drop_duplicates()\n",
    "demographics_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(demographics_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "There are two tables for final analyses. The immigration table includes the immigration records and the city table includes city population. At the end, two tables can be joined using i94addr from immigration table and state code from city table.\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Step 1: Read and load data into staging datafram using Spark: immigration_df_spark and city_df_spark.\n",
    "\n",
    "Step 2: Clean and transform staging datafram to analytics tables: immigration_table and city_table.\n",
    "\n",
    "Step 3: Output tables into parquet files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read immigration data\n",
    "immigration_df_spark =spark.read\\\n",
    "    .format('com.github.saurfang.sas.spark')\\\n",
    "    .load(immigration_fname)\n",
    "\n",
    "# extract columns to create immigration table\n",
    "immigration_table = immigration_df_spark\\\n",
    "    .selectExpr(\\\n",
    "        \"cicid as id\", \"i94yr as year\", \\\n",
    "        \"i94mon as month\", \\\n",
    "        \"i94addr as address\", \\\n",
    "        \"arrdate as arrive_date\", \\\n",
    "        \"depdate as depart_date\", \\\n",
    "        \"biryear as birth_year\", \"gender\")\\\n",
    "    .dropDuplicates()\n",
    "\n",
    "# write immigration table to parquet files\n",
    "immigration_table\\\n",
    "    .write\\\n",
    "    .partitionBy(\"year\", \"month\")\\\n",
    "    .parquet(\"sas_data\", 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read city data\n",
    "city_df_spark = spark\\\n",
    "    .read\\\n",
    "    .format(\"csv\")\\\n",
    "    .options(header='true', delimiter=';')\\\n",
    "    .load(demographics_fname)\n",
    "\n",
    "# extract columns to create city table\n",
    "city_table = city_df_spark\\\n",
    "    .selectExpr(\\\n",
    "        \"City as city\", \\\n",
    "        \"State as state\", \\\n",
    "        \"`Median Age` as median_age\", \\\n",
    "        \"`Male Population` as male_population\", \\\n",
    "        \"`Female Population` as female_population\", \\\n",
    "        \"`Total Population` as total_population\", \\\n",
    "        \"`State Code` as state_code\")\\\n",
    "    .dropDuplicates()\n",
    "\n",
    "# write city table to parquet files\n",
    "city_table\\\n",
    "    .write\\\n",
    "    .parquet(\"city_data\", 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read parquet data to perform quality checks\n",
    "immigration_check = spark.read.parquet(\"sas_data\")\n",
    "\n",
    "city_check = spark.read.parquet(\"city_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output sample data for immigration table\n",
    "immigration_check.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output sample data for city table\n",
    "city_check.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicates\n",
    "immigration_check.count() == len(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicates\n",
    "city_check.count() == len(demographics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique key\n",
    "immigration_check.select(\"id\").distinct().count() == len(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique key\n",
    "city_check.select(\"city\", \"state\").distinct().count() == len(demographics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "immigration table:\n",
    "* id - unique id for each record\n",
    "* year - the year of data\n",
    "* month - the month of data\n",
    "* address - the state\n",
    "* arrive_date - the arrive date\n",
    "* depart_date - the departure date\n",
    "* birth_year - the birth year which can be used to calculate age\n",
    "* gender - the gender\n",
    "\n",
    "city table:\n",
    "* city - city name\n",
    "* state - state name\n",
    "* median_age - median age of population\n",
    "* male_population - the number of male population\n",
    "* female_population - the number of femail population\n",
    "* total_population - the number of total population\n",
    "* state_code - the code of state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "        Spark can run analytics orders of magnitude faster than existing Hadoop deployments. This means more interactivity, faster experimentation, and increased productivity for analysts.\n",
    "\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "        Data should be updated in daily or monthly basis as the analytics job will be focus on the month or year dimension.\n",
    "\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "\n",
    "        Parameterize the ETL pipeline so that it can be run by Date (YYYY/MM).\n",
    "\n",
    "\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "        Improve the ETL pipeline to load new data in a daily basis and schedule it to run at 12am every day with a date parameter which should be the current date.\n",
    "\n",
    "\n",
    " * The database needed to be accessed by 100+ people.\n",
    "\n",
    "        Store data in separate tables using date parameter. For example, immigration data for 201604 will be stored in table immigration_201604. The goal is to reduce posibilities of running queries on one table which will be inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
